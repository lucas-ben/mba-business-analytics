{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb82fac1",
   "metadata": {},
   "source": [
    "- In OLS, there is a single solution that can be solved for (closed form solution)\n",
    "\n",
    "- Multicollinearity is a problem for linear regression\n",
    "- Without scaling data, coefficients can be untrustworthy (coefficients will change with any change to the model)\n",
    "\n",
    "- L1 and L2 are regularization methods that can be applied to linear models, deep learning models, SVM\n",
    "- Regularization means controlling for the coefficients I have in my model (brings everything to an equal playing field so I don't train a model that is too dependent on any one variable)\n",
    "\n",
    "- Foundational models: combine all the data a company has and train a model on it (a general use model) and can be used for clustering, classification, ...\n",
    "\n",
    "- Regularization is a way to drive feature selection; a way to remove variables that don't have a large impact on the model\n",
    "\n",
    "- Regularization isn't good for scalability (for production, would require retraining because \"dropping\" a variable today introduces bias and not being able to capture any future patterns for that variable if they do come up)\n",
    "\n",
    "- If I'm confident there is multicollinearity, it might be safe to say it won't change in the near future so it would be okay to use regularization\n",
    "\n",
    "- Regularization is feature selection (want to build a model and focus on the computation)\n",
    "\n",
    "- t is the number of optimal coefficients the model should keep (lasso regularization constraint)\n",
    "\n",
    "- The problem canbe framed as either a constraint (t) or penalty (alpha)\n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
